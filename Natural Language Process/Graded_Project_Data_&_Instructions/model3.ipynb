{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Team Members:\n",
    "- Amir Mobayen\n",
    "- Leelav Kareem\n",
    "- Nikita Chistyakov\n",
    "\n",
    "## Topic:\n",
    "fine-tunedTransformer Architecture from a pretrained model that can be found on sites like HuggingFace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 13:28:04.304772: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-23 13:28:04.306259: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-23 13:28:04.334373: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-23 13:28:04.334863: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-23 13:28:04.891616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from transformers import BertTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:05.709937529Z",
     "start_time": "2023-07-23T11:28:03.773254519Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About the dataset:\n",
    "List of tweet texts with emotion labels like joy, sadness, fear, anger…\n",
    "Dataset is split into train, test and validation sets for building the machine learning model. At first, you are\n",
    "given only train and test sets. The validation one will be given in the end of the project for you to check\n",
    "the final performance of your algorithm (to make sure there is no overfitting over the test data).\n",
    "You can work on this project on group of one, two or three students. This exercise is mandatory, not\n",
    "giving it back is equivalent to getting to lowest grade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Goal:\n",
    "- Train different kind of models able to classify each text according to the sentiment mainly present\n",
    "in it\n",
    "- Compare the results of your different models and try to analyze and explain the differences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_file = './NLP_exam_emotions_dataset/train.txt'\n",
    "test_flie = './NLP_exam_emotions_dataset/test.txt'\n",
    "\n",
    "max_len = 128  # Maximum sequence length for BERT"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:05.712724289Z",
     "start_time": "2023-07-23T11:28:05.710778877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(                             i didnt feel humiliated  sadness\n 0  i can go from feeling so hopeless to so damned...  sadness\n 1   im grabbing a minute to post i feel greedy wrong    anger\n 2  i am ever feeling nostalgic about the fireplac...     love\n 3                               i am feeling grouchy    anger\n 4  ive been feeling a little burdened lately wasn...  sadness,\n   im feeling rather rotten so im not very ambitious right now  sadness\n 0          im updating my blog because i feel shitty           sadness\n 1  i never make her separate from me because i do...           sadness\n 2  i left with my bouquet of red and yellow tulip...               joy\n 3    i was feeling a little vain when i did this one           sadness\n 4  i cant walk into a shop anywhere where i do no...              fear)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(data_file, delimiter=';', header=0)\n",
    "test_data = pd.read_csv(test_flie, delimiter=';', header=0)\n",
    "\n",
    "train_data.head(), test_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:05.743819671Z",
     "start_time": "2023-07-23T11:28:05.713260823Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68534c13950d4a6197da6c107339ad53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1405a5cd19eb47d4aaf870e50dca4407"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d628455775c4c029651cce8813c39bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:07.336605727Z",
     "start_time": "2023-07-23T11:28:05.744198376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_text(text, tokenizer, max_len):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length')\n",
    "    return input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:07.338842706Z",
     "start_time": "2023-07-23T11:28:07.337499276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 13:28:11.155955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-23 13:28:11.156354: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_input_ids_list = [preprocess_text(text, tokenizer, max_len) for text in train_data.iloc[:, 0]]\n",
    "train_input_ids_batch = tf.convert_to_tensor(train_input_ids_list)\n",
    "train_labels = tf.convert_to_tensor(train_data.iloc[:, 1])\n",
    "len_label = len(np.unique(train_labels))\n",
    "\n",
    "test_input_ids_list = [preprocess_text(text, tokenizer, max_len) for text in test_data.iloc[:, 0]]\n",
    "test_input_ids_batch = tf.convert_to_tensor(test_input_ids_list)\n",
    "test_labels = tf.convert_to_tensor(test_data.iloc[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:11.581631832Z",
     "start_time": "2023-07-23T11:28:07.340287279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_y = label_encoder.fit_transform(train_data.iloc[:, 1])\n",
    "test_y = label_encoder.transform(test_data.iloc[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:11.586823473Z",
     "start_time": "2023-07-23T11:28:11.585103009Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bedec3f4ddc345d2b081fd115b6b48de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<bound method Model.summary of <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x7f2958fbc150>>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len_label)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T11:28:22.210606610Z",
     "start_time": "2023-07-23T11:28:11.587475283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "139/500 [=======>......................] - ETA: 52:12 - loss: 1.5829 - accuracy: 0.3357"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_input_ids_batch, train_y, epochs=5, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-23T11:28:22.213251897Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predicted_class_indices = np.argmax(model.predict(test_input_ids_batch)[0], axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_class_indices)\n",
    "accuracy = accuracy_score(test_data.iloc[:, 1], predicted_labels)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(test_data.iloc[:, 1], predicted_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.columns = [\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n",
    "    hist_df.index = np.arange(1, len(hist_df) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_accuracy, lw=3, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.accuracy, lw=3, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=3, label='Validation Loss')\n",
    "    axs[1].plot(hist_df.loss, lw=3, label='Training Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "plot_results(history)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "sample_range = 10\n",
    "for i in range(sample_range):\n",
    "    sample = test_data.iloc[i, 0]\n",
    "    sample_test, _, _ = tokenizer([sample])  # Tokenize the sample as a list\n",
    "\n",
    "    sample_test = pad_sequences(sample_test, maxlen=max_len)  # Pad the sequence to match max_len\n",
    "    test_predict = model.predict(sample_test)\n",
    "    prediction = np.argmax(test_predict)\n",
    "\n",
    "    # After obtaining the predicted class index\n",
    "    predicted_label = label_encoder.inverse_transform([prediction])[0]\n",
    "    true_label = test_data.iloc[i, 1]\n",
    "    if predicted_label == true_label:\n",
    "        count += 1\n",
    "    print(f'Predicted class vs True Class: [{predicted_label}, {true_label}]')\n",
    "\n",
    "print(f'\\nsample accuracy = {count / sample_range}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
