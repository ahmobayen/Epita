{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MidTerm Exam\n",
    "Deadline : 02/06/23\n",
    "Amir Mobayen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and train a neural network on the dataset provided using the Tensorflow-Keras framework. You should:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from numpy import ravel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:36.931585800Z",
     "start_time": "2023-05-31T19:28:31.828346800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "     sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  caa   \nage                                                                           \n63     1   3     145   233    1        0       150     0      2.3    0    0  \\\n37     1   2     130   250    0        1       187     0      3.5    0    0   \n41     0   1     130   204    0        0       172     0      1.4    2    0   \n56     1   1     120   236    0        1       178     0      0.8    2    0   \n57     0   0     120   354    0        1       163     1      0.6    2    0   \n57     1   0     140   192    0        1       148     0      0.4    1    0   \n56     0   1     140   294    0        0       153     0      1.3    1    0   \n44     1   1     120   263    0        1       173     0      0.0    2    0   \n52     1   2     172   199    1        1       162     0      0.5    2    0   \n57     1   2     150   168    0        1       174     0      1.6    2    0   \n\n     thall  output  \nage                 \n63       1       1  \n37       2       1  \n41       2       1  \n56       2       1  \n57       2       1  \n57       1       1  \n56       2       1  \n44       3       1  \n52       3       1  \n57       2       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trtbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalachh</th>\n      <th>exng</th>\n      <th>oldpeak</th>\n      <th>slp</th>\n      <th>caa</th>\n      <th>thall</th>\n      <th>output</th>\n    </tr>\n    <tr>\n      <th>age</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>1</td>\n      <td>3</td>\n      <td>145</td>\n      <td>233</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>2.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>236</td>\n      <td>0</td>\n      <td>1</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>1</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>192</td>\n      <td>0</td>\n      <td>1</td>\n      <td>148</td>\n      <td>0</td>\n      <td>0.4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>0</td>\n      <td>1</td>\n      <td>140</td>\n      <td>294</td>\n      <td>0</td>\n      <td>0</td>\n      <td>153</td>\n      <td>0</td>\n      <td>1.3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>263</td>\n      <td>0</td>\n      <td>1</td>\n      <td>173</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>1</td>\n      <td>2</td>\n      <td>172</td>\n      <td>199</td>\n      <td>1</td>\n      <td>1</td>\n      <td>162</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>1</td>\n      <td>2</td>\n      <td>150</td>\n      <td>168</td>\n      <td>0</td>\n      <td>1</td>\n      <td>174</td>\n      <td>0</td>\n      <td>1.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"heart.csv\", index_col=0)\n",
    "data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:36.980152300Z",
     "start_time": "2023-05-31T19:28:36.934587400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Small explanation about the features:\n",
    "age : Age of the patient\n",
    "sex : Sex of the patient\n",
    "cp : Chest Pain type chest pain type\n",
    "Value 1: typical angina\n",
    "Value 2: atypical angina\n",
    "Value 3: non-anginal pain\n",
    "Value 4: asymptomatic\n",
    "trtbps : resting blood pressure (in mm Hg)chol : cholestoral in mg/dl fetched via BMI sensorfbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)restecg :\n",
    "\n",
    "resting electrocardiographic results\n",
    "Value 0: normal\n",
    "Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "thalachh : maximum heart rate achievedexng : exercise induced angina (1 = yes; 0 = no)oldpeak : ST depression induced by exercise relative to restslp : the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)caa : number of major vessels (0-3)thall : 2 = normal; 1 = fixed defect; 3 = reversable defectTarget:\n",
    "output : 0= less chance of heart attack 1= more chance of heart attack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "((302, 12), (302, 1))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a copy in order not to touch the original file\n",
    "data_set = data.copy()\n",
    "\n",
    "# clean input data:\n",
    "# remove null values on numerical columns\n",
    "data_set = data_set.fillna(data_set.median())\n",
    "\n",
    "# remove duplicates\n",
    "data_set = data_set.drop_duplicates()\n",
    "\n",
    "# separate features and label\n",
    "y = data_set.pop('output')\n",
    "X = data_set\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.073252400Z",
     "start_time": "2023-05-31T19:28:36.967146100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a test set:\n",
    "- for a regression task: use 20% of the data and set the random seed to 42\n",
    "- for a classification task: use 20% of the data, set the random seed to 42 and use a stratified splitting method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def split(X, y, stratify=False):\n",
    "    if stratify:\n",
    "        train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    else:\n",
    "        train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.1 / (1 - 0.2), random_state=42)\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape, val_X.shape, val_y.shape)\n",
    "    return train_X, train_y, test_X, test_y, val_X, val_y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.181526200Z",
     "start_time": "2023-05-31T19:28:37.009194500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 12) (210, 1) (61, 12) (61, 1) (31, 12) (31, 1)\n",
      "(210, 12) (210, 1) (61, 12) (61, 1) (31, 12) (31, 1)\n"
     ]
    }
   ],
   "source": [
    "# Regression Task:\n",
    "reg_train_X, reg_train_y, reg_test_X, reg_test_y, reg_val_X, reg_val_y = split(X, y, False)\n",
    "\n",
    "# classification Task:\n",
    "class_train_X, class_train_y, class_test_X, class_test_y, class_val_X, class_val_y = split(X, y, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.272875300Z",
     "start_time": "2023-05-31T19:28:37.013147900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess the data:\n",
    "While preprocessing is not the subject of this exam, wrong/poor preprocessing steps will be sanctioned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def preprocess_data(train_X, train_y, test_X, test_y, val_X, val_y):\n",
    "    #  performs data preprocessing on the provided training, testing, and validation datasets.\n",
    "    # standardize numerical features\n",
    "    std = StandardScaler()\n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    # Identifies the numerical columns in the training dataset.\n",
    "    num_col = train_X.select_dtypes(include='number').columns\n",
    "\n",
    "    # applies the standardization to the numerical columns.\n",
    "    ct = ColumnTransformer([\n",
    "        ('num', std, num_col),\n",
    "    ])\n",
    "\n",
    "    # Standardizing the Features\n",
    "    X_train_scl = ct.fit_transform(train_X)\n",
    "    X_val_scl = ct.transform(val_X)\n",
    "    X_test_scl = ct.transform(test_X)\n",
    "\n",
    "    # Encoding the Target Variable\n",
    "    y_train_scl = lbl.fit_transform(ravel(train_y))\n",
    "    y_val_scl = lbl.transform(ravel(val_y))\n",
    "    y_test_scl = lbl.transform(ravel(test_y))\n",
    "\n",
    "    return X_train_scl, y_train_scl, X_test_scl, y_test_scl, X_val_scl, y_val_scl\n",
    "\n",
    "\n",
    "reg_train_X, reg_train_y, reg_test_X, reg_test_y, reg_val_X, reg_val_y = preprocess_data(reg_train_X, reg_train_y,\n",
    "                                                                                         reg_test_X, reg_test_y,\n",
    "                                                                                         reg_val_X, reg_val_y)\n",
    "\n",
    "class_train_X, class_train_y, class_test_X, class_test_y, class_val_X, class_val_y = preprocess_data(class_train_X,\n",
    "                                                                                                     class_train_y,\n",
    "                                                                                                     class_test_X,\n",
    "                                                                                                     class_test_y,\n",
    "                                                                                                     class_val_X,\n",
    "                                                                                                     class_val_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.275889700Z",
     "start_time": "2023-05-31T19:28:37.040154300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "CLASSIFICATION = True\n",
    "if CLASSIFICATION:\n",
    "    X_train_scl, y_train_scl, X_test_scl, y_test_scl, X_val_scl, y_val_scl = class_train_X, class_train_y, class_test_X, class_test_y, class_val_X, class_val_y\n",
    "else:\n",
    "    X_train_scl, y_train_scl, X_test_scl, y_test_scl, X_val_scl, y_val_scl = reg_train_X, reg_train_y, reg_test_X, reg_test_y, reg_val_X, reg_val_y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.277892600Z",
     "start_time": "2023-05-31T19:28:37.085797500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Measuring Execution Time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = datetime.now()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = datetime.now()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time.total_seconds()} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.277892600Z",
     "start_time": "2023-05-31T19:28:37.098792500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STUDY"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:31:13.687368Z",
     "start_time": "2023-05-31T18:31:13.523537900Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "regression_looses = [tf.keras.losses.mean_squared_error,\n",
    "                     # Useful when outliers are rare but need to be taken into account\n",
    "                     tf.keras.losses.mean_absolute_error,  # Useful when outliers are common,when the data is very noisy\n",
    "                     tf.keras.losses.huber,\n",
    "                     # Useful when outliers exist, but most of the data follows a normal distribution\n",
    "                     tf.keras.losses.log_cosh,\n",
    "                     # Useful when outliers exist, but most of the data follows a normal distribution\n",
    "                     ]\n",
    "# This is a classification problem (goal is yes or no) so using regression_looses is useless. I will keep it just for a small test if my prediction with classification was not correct !\n",
    "\n",
    "# We will not use (Sparse) Categorical Cross-Entropy -> it's for more than two output classes\n",
    "\n",
    "binary_loss = [tf.keras.losses.BinaryCrossentropy(),  # binary classification problems\n",
    "               tf.keras.losses.hinge,  # Useful for binary classification problems with outliers or unbalanced classes\n",
    "               ]\n",
    "\n",
    "kernel_initializer = ['glorot', 'he_normal', 'lecun']\n",
    "activation_function = ['relu', 'leaky_relu', 'rrelu', 'prelu', 'elu', 'selu']\n",
    "\n",
    "optimizers = [tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.01, nesterov=True),  # Good for quallity\n",
    "              tf.keras.optimizers.RMSprop,  # more robust than adagrad\n",
    "              tf.keras.optimizers.Adam(learning_rate=0.01)  # combination of RMSprop and SGD -> default\n",
    "              ]\n",
    "\n",
    "\n",
    "# leaky relu is better than relu\n",
    "# selu -> maximize out put but slow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.298426500Z",
     "start_time": "2023-05-31T19:28:37.113312100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "several solutions to the vanishing and exploding gradient problem in deep neural networks:\n",
    "- Using a better weight initialization method\n",
    "- Using alternative activation functions\n",
    "- Using gradient clipping methods\n",
    "- Using batch normalization\n",
    "- Building networks with skip connections\n",
    "- Creating new types of neurons"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:31:13.687368Z",
     "start_time": "2023-05-31T18:31:13.557146500Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Glorot initialization is not good with RELU Activation!\n",
    "# LeCun initialization is generally used in convolutional neural networks and paired with the SELU\n",
    "\n",
    "# relu and its variant -> he\n",
    "# selu -> lecun\n",
    "# rest -> glorot\n",
    "\n",
    "## Methods to prevent overfitting\n",
    "# EARLY STOPPING\n",
    "# REGULARIZATION:\n",
    "# penalty term encourages the model to learn simpler and more generalizable patterns by adding a cost to more complex models.\n",
    "# Drop out\n",
    "# tf.keras.layers.Dense(16, kernel_initializer='he_normal', activation='relu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.299426Z",
     "start_time": "2023-05-31T19:28:37.126371600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:31:13.688392700Z",
     "start_time": "2023-05-31T18:31:13.569282Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "1- Early Stop is going to use in all models -> for better estimation and reducing the number of repititions.\n",
    "2- Epochs -> consider 100 which is seems to be too much for 300 rows.\n",
    "Update -> 100 seems not enough because early stop didnt active and chart still have space to go! -> try 200"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "@measure_execution_time\n",
    "def build_model(model, epochs, loss, optimizer, metric, batch_size=32, save=False):\n",
    "    tf.keras.backend.clear_session()\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[metric])\n",
    "    model.summary()\n",
    "    if save:\n",
    "        model.save('model')\n",
    "    history = model.fit(X_train_scl, y_train_scl,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_val_scl, y_val_scl),\n",
    "                        callbacks=[early_stopping_cb])\n",
    "    return history, early_stopping_cb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:37.299933400Z",
     "start_time": "2023-05-31T19:28:37.141920500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__PERFORMANCE MODEL__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 185\n",
      "Trainable params: 185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "7/7 [==============================] - 1s 38ms/step - loss: 0.7920 - binary_accuracy: 0.4619 - val_loss: 0.7241 - val_binary_accuracy: 0.4839\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7705 - binary_accuracy: 0.4667 - val_loss: 0.7037 - val_binary_accuracy: 0.5161\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7515 - binary_accuracy: 0.4667 - val_loss: 0.6863 - val_binary_accuracy: 0.5806\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7348 - binary_accuracy: 0.4571 - val_loss: 0.6702 - val_binary_accuracy: 0.5806\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7196 - binary_accuracy: 0.4667 - val_loss: 0.6555 - val_binary_accuracy: 0.5806\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7064 - binary_accuracy: 0.4619 - val_loss: 0.6422 - val_binary_accuracy: 0.5806\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6938 - binary_accuracy: 0.4619 - val_loss: 0.6307 - val_binary_accuracy: 0.5806\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6832 - binary_accuracy: 0.4762 - val_loss: 0.6193 - val_binary_accuracy: 0.5806\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6735 - binary_accuracy: 0.5000 - val_loss: 0.6090 - val_binary_accuracy: 0.5806\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6644 - binary_accuracy: 0.5286 - val_loss: 0.5994 - val_binary_accuracy: 0.6129\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6566 - binary_accuracy: 0.5762 - val_loss: 0.5904 - val_binary_accuracy: 0.5806\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6491 - binary_accuracy: 0.5810 - val_loss: 0.5815 - val_binary_accuracy: 0.6774\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6419 - binary_accuracy: 0.6095 - val_loss: 0.5732 - val_binary_accuracy: 0.6774\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6351 - binary_accuracy: 0.6286 - val_loss: 0.5652 - val_binary_accuracy: 0.6774\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6288 - binary_accuracy: 0.6571 - val_loss: 0.5575 - val_binary_accuracy: 0.6774\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6229 - binary_accuracy: 0.6667 - val_loss: 0.5502 - val_binary_accuracy: 0.6774\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6169 - binary_accuracy: 0.6810 - val_loss: 0.5428 - val_binary_accuracy: 0.6774\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6113 - binary_accuracy: 0.6952 - val_loss: 0.5358 - val_binary_accuracy: 0.7097\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6058 - binary_accuracy: 0.7095 - val_loss: 0.5292 - val_binary_accuracy: 0.7419\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6007 - binary_accuracy: 0.7190 - val_loss: 0.5226 - val_binary_accuracy: 0.7419\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5952 - binary_accuracy: 0.7238 - val_loss: 0.5164 - val_binary_accuracy: 0.7419\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5902 - binary_accuracy: 0.7286 - val_loss: 0.5104 - val_binary_accuracy: 0.7742\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5853 - binary_accuracy: 0.7238 - val_loss: 0.5044 - val_binary_accuracy: 0.7742\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5806 - binary_accuracy: 0.7286 - val_loss: 0.4987 - val_binary_accuracy: 0.7742\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5758 - binary_accuracy: 0.7333 - val_loss: 0.4929 - val_binary_accuracy: 0.8065\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5711 - binary_accuracy: 0.7476 - val_loss: 0.4873 - val_binary_accuracy: 0.8065\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5667 - binary_accuracy: 0.7524 - val_loss: 0.4817 - val_binary_accuracy: 0.8065\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5624 - binary_accuracy: 0.7476 - val_loss: 0.4765 - val_binary_accuracy: 0.8065\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5582 - binary_accuracy: 0.7476 - val_loss: 0.4712 - val_binary_accuracy: 0.8065\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5540 - binary_accuracy: 0.7571 - val_loss: 0.4660 - val_binary_accuracy: 0.8065\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5495 - binary_accuracy: 0.7619 - val_loss: 0.4611 - val_binary_accuracy: 0.8387\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5456 - binary_accuracy: 0.7667 - val_loss: 0.4562 - val_binary_accuracy: 0.8387\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5419 - binary_accuracy: 0.7667 - val_loss: 0.4514 - val_binary_accuracy: 0.8387\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5377 - binary_accuracy: 0.7714 - val_loss: 0.4467 - val_binary_accuracy: 0.8387\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5341 - binary_accuracy: 0.7714 - val_loss: 0.4423 - val_binary_accuracy: 0.8387\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5301 - binary_accuracy: 0.7714 - val_loss: 0.4379 - val_binary_accuracy: 0.8387\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5267 - binary_accuracy: 0.7714 - val_loss: 0.4335 - val_binary_accuracy: 0.8387\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5229 - binary_accuracy: 0.7810 - val_loss: 0.4292 - val_binary_accuracy: 0.8387\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5195 - binary_accuracy: 0.7857 - val_loss: 0.4251 - val_binary_accuracy: 0.8387\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5159 - binary_accuracy: 0.7905 - val_loss: 0.4211 - val_binary_accuracy: 0.8387\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5126 - binary_accuracy: 0.7952 - val_loss: 0.4166 - val_binary_accuracy: 0.8387\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5092 - binary_accuracy: 0.8095 - val_loss: 0.4125 - val_binary_accuracy: 0.8387\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5058 - binary_accuracy: 0.8095 - val_loss: 0.4083 - val_binary_accuracy: 0.8387\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5025 - binary_accuracy: 0.8190 - val_loss: 0.4047 - val_binary_accuracy: 0.8387\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4992 - binary_accuracy: 0.8238 - val_loss: 0.4010 - val_binary_accuracy: 0.8387\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4963 - binary_accuracy: 0.8238 - val_loss: 0.3976 - val_binary_accuracy: 0.8387\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4932 - binary_accuracy: 0.8286 - val_loss: 0.3942 - val_binary_accuracy: 0.8387\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4904 - binary_accuracy: 0.8286 - val_loss: 0.3906 - val_binary_accuracy: 0.8387\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4873 - binary_accuracy: 0.8286 - val_loss: 0.3874 - val_binary_accuracy: 0.8387\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4844 - binary_accuracy: 0.8286 - val_loss: 0.3840 - val_binary_accuracy: 0.8387\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4817 - binary_accuracy: 0.8286 - val_loss: 0.3807 - val_binary_accuracy: 0.8387\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4788 - binary_accuracy: 0.8286 - val_loss: 0.3775 - val_binary_accuracy: 0.8387\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4761 - binary_accuracy: 0.8333 - val_loss: 0.3745 - val_binary_accuracy: 0.8710\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4735 - binary_accuracy: 0.8333 - val_loss: 0.3718 - val_binary_accuracy: 0.8710\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4709 - binary_accuracy: 0.8333 - val_loss: 0.3689 - val_binary_accuracy: 0.8710\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4685 - binary_accuracy: 0.8333 - val_loss: 0.3662 - val_binary_accuracy: 0.8387\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4660 - binary_accuracy: 0.8381 - val_loss: 0.3634 - val_binary_accuracy: 0.8387\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4634 - binary_accuracy: 0.8286 - val_loss: 0.3606 - val_binary_accuracy: 0.8387\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4610 - binary_accuracy: 0.8333 - val_loss: 0.3581 - val_binary_accuracy: 0.8710\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4588 - binary_accuracy: 0.8381 - val_loss: 0.3556 - val_binary_accuracy: 0.8710\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4563 - binary_accuracy: 0.8381 - val_loss: 0.3530 - val_binary_accuracy: 0.8710\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4540 - binary_accuracy: 0.8381 - val_loss: 0.3507 - val_binary_accuracy: 0.8710\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4518 - binary_accuracy: 0.8381 - val_loss: 0.3485 - val_binary_accuracy: 0.8710\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4496 - binary_accuracy: 0.8381 - val_loss: 0.3462 - val_binary_accuracy: 0.8710\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4474 - binary_accuracy: 0.8429 - val_loss: 0.3439 - val_binary_accuracy: 0.8710\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4452 - binary_accuracy: 0.8429 - val_loss: 0.3417 - val_binary_accuracy: 0.8710\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4430 - binary_accuracy: 0.8476 - val_loss: 0.3395 - val_binary_accuracy: 0.8710\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4410 - binary_accuracy: 0.8476 - val_loss: 0.3374 - val_binary_accuracy: 0.8710\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4389 - binary_accuracy: 0.8476 - val_loss: 0.3355 - val_binary_accuracy: 0.8710\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4370 - binary_accuracy: 0.8476 - val_loss: 0.3334 - val_binary_accuracy: 0.8710\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4349 - binary_accuracy: 0.8476 - val_loss: 0.3314 - val_binary_accuracy: 0.8710\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4331 - binary_accuracy: 0.8476 - val_loss: 0.3296 - val_binary_accuracy: 0.8710\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4312 - binary_accuracy: 0.8476 - val_loss: 0.3277 - val_binary_accuracy: 0.8710\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4291 - binary_accuracy: 0.8476 - val_loss: 0.3258 - val_binary_accuracy: 0.8710\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4273 - binary_accuracy: 0.8476 - val_loss: 0.3239 - val_binary_accuracy: 0.8710\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4255 - binary_accuracy: 0.8476 - val_loss: 0.3224 - val_binary_accuracy: 0.8710\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4236 - binary_accuracy: 0.8476 - val_loss: 0.3209 - val_binary_accuracy: 0.8710\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4221 - binary_accuracy: 0.8476 - val_loss: 0.3193 - val_binary_accuracy: 0.8710\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4203 - binary_accuracy: 0.8476 - val_loss: 0.3178 - val_binary_accuracy: 0.8710\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4187 - binary_accuracy: 0.8476 - val_loss: 0.3158 - val_binary_accuracy: 0.8710\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4169 - binary_accuracy: 0.8476 - val_loss: 0.3144 - val_binary_accuracy: 0.8710\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4154 - binary_accuracy: 0.8476 - val_loss: 0.3127 - val_binary_accuracy: 0.8710\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4136 - binary_accuracy: 0.8524 - val_loss: 0.3111 - val_binary_accuracy: 0.8710\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4120 - binary_accuracy: 0.8524 - val_loss: 0.3098 - val_binary_accuracy: 0.8710\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4105 - binary_accuracy: 0.8524 - val_loss: 0.3084 - val_binary_accuracy: 0.8710\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4089 - binary_accuracy: 0.8524 - val_loss: 0.3070 - val_binary_accuracy: 0.8710\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4074 - binary_accuracy: 0.8524 - val_loss: 0.3057 - val_binary_accuracy: 0.8710\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4059 - binary_accuracy: 0.8524 - val_loss: 0.3047 - val_binary_accuracy: 0.8710\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4046 - binary_accuracy: 0.8571 - val_loss: 0.3035 - val_binary_accuracy: 0.8710\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4031 - binary_accuracy: 0.8571 - val_loss: 0.3020 - val_binary_accuracy: 0.8710\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4016 - binary_accuracy: 0.8571 - val_loss: 0.3005 - val_binary_accuracy: 0.8710\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4002 - binary_accuracy: 0.8571 - val_loss: 0.2993 - val_binary_accuracy: 0.8710\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3988 - binary_accuracy: 0.8571 - val_loss: 0.2981 - val_binary_accuracy: 0.8710\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3975 - binary_accuracy: 0.8571 - val_loss: 0.2973 - val_binary_accuracy: 0.8710\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3961 - binary_accuracy: 0.8571 - val_loss: 0.2962 - val_binary_accuracy: 0.8710\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3949 - binary_accuracy: 0.8571 - val_loss: 0.2950 - val_binary_accuracy: 0.8710\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3935 - binary_accuracy: 0.8571 - val_loss: 0.2941 - val_binary_accuracy: 0.8710\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3923 - binary_accuracy: 0.8571 - val_loss: 0.2933 - val_binary_accuracy: 0.8710\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3912 - binary_accuracy: 0.8571 - val_loss: 0.2923 - val_binary_accuracy: 0.8710\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3899 - binary_accuracy: 0.8571 - val_loss: 0.2913 - val_binary_accuracy: 0.8710\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3887 - binary_accuracy: 0.8571 - val_loss: 0.2902 - val_binary_accuracy: 0.8710\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3874 - binary_accuracy: 0.8571 - val_loss: 0.2894 - val_binary_accuracy: 0.8710\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3864 - binary_accuracy: 0.8571 - val_loss: 0.2886 - val_binary_accuracy: 0.8710\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3851 - binary_accuracy: 0.8571 - val_loss: 0.2876 - val_binary_accuracy: 0.8710\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3841 - binary_accuracy: 0.8571 - val_loss: 0.2870 - val_binary_accuracy: 0.8710\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3829 - binary_accuracy: 0.8571 - val_loss: 0.2860 - val_binary_accuracy: 0.8710\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3818 - binary_accuracy: 0.8571 - val_loss: 0.2853 - val_binary_accuracy: 0.8710\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3807 - binary_accuracy: 0.8571 - val_loss: 0.2845 - val_binary_accuracy: 0.8710\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3798 - binary_accuracy: 0.8571 - val_loss: 0.2840 - val_binary_accuracy: 0.8710\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3786 - binary_accuracy: 0.8571 - val_loss: 0.2835 - val_binary_accuracy: 0.8710\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3775 - binary_accuracy: 0.8571 - val_loss: 0.2828 - val_binary_accuracy: 0.8710\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3766 - binary_accuracy: 0.8571 - val_loss: 0.2821 - val_binary_accuracy: 0.8710\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3756 - binary_accuracy: 0.8571 - val_loss: 0.2814 - val_binary_accuracy: 0.8710\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3747 - binary_accuracy: 0.8571 - val_loss: 0.2806 - val_binary_accuracy: 0.9032\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3736 - binary_accuracy: 0.8571 - val_loss: 0.2798 - val_binary_accuracy: 0.9032\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3725 - binary_accuracy: 0.8619 - val_loss: 0.2793 - val_binary_accuracy: 0.9032\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3716 - binary_accuracy: 0.8571 - val_loss: 0.2786 - val_binary_accuracy: 0.9032\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3705 - binary_accuracy: 0.8571 - val_loss: 0.2782 - val_binary_accuracy: 0.9032\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3696 - binary_accuracy: 0.8571 - val_loss: 0.2778 - val_binary_accuracy: 0.9032\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3688 - binary_accuracy: 0.8524 - val_loss: 0.2773 - val_binary_accuracy: 0.9032\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3676 - binary_accuracy: 0.8571 - val_loss: 0.2767 - val_binary_accuracy: 0.9032\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3667 - binary_accuracy: 0.8619 - val_loss: 0.2764 - val_binary_accuracy: 0.9032\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3657 - binary_accuracy: 0.8619 - val_loss: 0.2762 - val_binary_accuracy: 0.9032\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3650 - binary_accuracy: 0.8524 - val_loss: 0.2755 - val_binary_accuracy: 0.9032\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3641 - binary_accuracy: 0.8667 - val_loss: 0.2750 - val_binary_accuracy: 0.9032\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3631 - binary_accuracy: 0.8667 - val_loss: 0.2749 - val_binary_accuracy: 0.9032\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3623 - binary_accuracy: 0.8667 - val_loss: 0.2746 - val_binary_accuracy: 0.9032\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3613 - binary_accuracy: 0.8667 - val_loss: 0.2742 - val_binary_accuracy: 0.9032\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3604 - binary_accuracy: 0.8667 - val_loss: 0.2738 - val_binary_accuracy: 0.9032\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3596 - binary_accuracy: 0.8667 - val_loss: 0.2735 - val_binary_accuracy: 0.9032\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3586 - binary_accuracy: 0.8667 - val_loss: 0.2732 - val_binary_accuracy: 0.9032\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3577 - binary_accuracy: 0.8714 - val_loss: 0.2729 - val_binary_accuracy: 0.9032\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3569 - binary_accuracy: 0.8667 - val_loss: 0.2725 - val_binary_accuracy: 0.9032\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3561 - binary_accuracy: 0.8667 - val_loss: 0.2721 - val_binary_accuracy: 0.9032\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3551 - binary_accuracy: 0.8667 - val_loss: 0.2716 - val_binary_accuracy: 0.9032\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3543 - binary_accuracy: 0.8667 - val_loss: 0.2715 - val_binary_accuracy: 0.9032\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3537 - binary_accuracy: 0.8667 - val_loss: 0.2711 - val_binary_accuracy: 0.9032\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3528 - binary_accuracy: 0.8667 - val_loss: 0.2709 - val_binary_accuracy: 0.9032\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3519 - binary_accuracy: 0.8667 - val_loss: 0.2707 - val_binary_accuracy: 0.9032\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3512 - binary_accuracy: 0.8714 - val_loss: 0.2704 - val_binary_accuracy: 0.9032\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3507 - binary_accuracy: 0.8714 - val_loss: 0.2707 - val_binary_accuracy: 0.9032\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3497 - binary_accuracy: 0.8667 - val_loss: 0.2704 - val_binary_accuracy: 0.9032\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3489 - binary_accuracy: 0.8714 - val_loss: 0.2703 - val_binary_accuracy: 0.9032\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3480 - binary_accuracy: 0.8667 - val_loss: 0.2701 - val_binary_accuracy: 0.9032\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3473 - binary_accuracy: 0.8667 - val_loss: 0.2700 - val_binary_accuracy: 0.8710\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3466 - binary_accuracy: 0.8714 - val_loss: 0.2696 - val_binary_accuracy: 0.9032\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3458 - binary_accuracy: 0.8714 - val_loss: 0.2692 - val_binary_accuracy: 0.9032\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3452 - binary_accuracy: 0.8714 - val_loss: 0.2690 - val_binary_accuracy: 0.9032\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3444 - binary_accuracy: 0.8714 - val_loss: 0.2689 - val_binary_accuracy: 0.9032\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3436 - binary_accuracy: 0.8714 - val_loss: 0.2685 - val_binary_accuracy: 0.9032\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3431 - binary_accuracy: 0.8714 - val_loss: 0.2684 - val_binary_accuracy: 0.9032\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3422 - binary_accuracy: 0.8714 - val_loss: 0.2681 - val_binary_accuracy: 0.9032\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3418 - binary_accuracy: 0.8714 - val_loss: 0.2676 - val_binary_accuracy: 0.9032\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3411 - binary_accuracy: 0.8762 - val_loss: 0.2676 - val_binary_accuracy: 0.9032\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3403 - binary_accuracy: 0.8762 - val_loss: 0.2676 - val_binary_accuracy: 0.9032\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3396 - binary_accuracy: 0.8762 - val_loss: 0.2675 - val_binary_accuracy: 0.9032\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3391 - binary_accuracy: 0.8762 - val_loss: 0.2675 - val_binary_accuracy: 0.9032\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3384 - binary_accuracy: 0.8762 - val_loss: 0.2671 - val_binary_accuracy: 0.9032\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3377 - binary_accuracy: 0.8762 - val_loss: 0.2670 - val_binary_accuracy: 0.9032\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3371 - binary_accuracy: 0.8762 - val_loss: 0.2668 - val_binary_accuracy: 0.9032\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3364 - binary_accuracy: 0.8762 - val_loss: 0.2669 - val_binary_accuracy: 0.9032\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3357 - binary_accuracy: 0.8810 - val_loss: 0.2668 - val_binary_accuracy: 0.9032\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3353 - binary_accuracy: 0.8762 - val_loss: 0.2670 - val_binary_accuracy: 0.9032\n",
      "Elapsed time: 7.427736 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Large: speed and faster, small: generalization\n",
    "epochs = 200\n",
    "hidden_layer_unit = 8  # Total number of 300 record! First try is 8!\n",
    "hidden_layer_activation = 'relu'  # This is a hidden layer, Relu is good choice for this section\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(X_train_scl.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=hidden_layer_unit, activation=hidden_layer_activation),\n",
    "    tf.keras.layers.Dense(units=hidden_layer_unit, activation=hidden_layer_activation),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    # This is classification output -> Sigmoid or softmax are the best chooses\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "history1 ,early_stop1 = build_model(model1, epochs, loss, optimizer, metric ,batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:44.801004400Z",
     "start_time": "2023-05-31T19:28:37.209683400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                208       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 497\n",
      "Trainable params: 497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.7511 - binary_accuracy: 0.5286 - val_loss: 0.7378 - val_binary_accuracy: 0.5161\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7230 - binary_accuracy: 0.5476 - val_loss: 0.7096 - val_binary_accuracy: 0.5161\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6993 - binary_accuracy: 0.5571 - val_loss: 0.6855 - val_binary_accuracy: 0.5806\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6787 - binary_accuracy: 0.5952 - val_loss: 0.6658 - val_binary_accuracy: 0.5806\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6622 - binary_accuracy: 0.6333 - val_loss: 0.6470 - val_binary_accuracy: 0.6774\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6461 - binary_accuracy: 0.6714 - val_loss: 0.6318 - val_binary_accuracy: 0.6774\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6323 - binary_accuracy: 0.6714 - val_loss: 0.6162 - val_binary_accuracy: 0.6774\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6182 - binary_accuracy: 0.6857 - val_loss: 0.6015 - val_binary_accuracy: 0.6774\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6048 - binary_accuracy: 0.7095 - val_loss: 0.5878 - val_binary_accuracy: 0.6774\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5921 - binary_accuracy: 0.7333 - val_loss: 0.5729 - val_binary_accuracy: 0.7742\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5789 - binary_accuracy: 0.7524 - val_loss: 0.5596 - val_binary_accuracy: 0.7742\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5671 - binary_accuracy: 0.7667 - val_loss: 0.5489 - val_binary_accuracy: 0.7742\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5576 - binary_accuracy: 0.7810 - val_loss: 0.5366 - val_binary_accuracy: 0.8065\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5469 - binary_accuracy: 0.7905 - val_loss: 0.5247 - val_binary_accuracy: 0.8065\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5368 - binary_accuracy: 0.7857 - val_loss: 0.5141 - val_binary_accuracy: 0.8065\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5275 - binary_accuracy: 0.8048 - val_loss: 0.5088 - val_binary_accuracy: 0.8065\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5212 - binary_accuracy: 0.7810 - val_loss: 0.4975 - val_binary_accuracy: 0.8065\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5109 - binary_accuracy: 0.8048 - val_loss: 0.4854 - val_binary_accuracy: 0.8065\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5005 - binary_accuracy: 0.8238 - val_loss: 0.4750 - val_binary_accuracy: 0.8065\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4915 - binary_accuracy: 0.8190 - val_loss: 0.4679 - val_binary_accuracy: 0.8387\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4849 - binary_accuracy: 0.8238 - val_loss: 0.4591 - val_binary_accuracy: 0.8710\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4764 - binary_accuracy: 0.8190 - val_loss: 0.4505 - val_binary_accuracy: 0.8710\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4682 - binary_accuracy: 0.8190 - val_loss: 0.4417 - val_binary_accuracy: 0.8710\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4598 - binary_accuracy: 0.8190 - val_loss: 0.4336 - val_binary_accuracy: 0.8710\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4523 - binary_accuracy: 0.8286 - val_loss: 0.4263 - val_binary_accuracy: 0.8710\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4450 - binary_accuracy: 0.8286 - val_loss: 0.4203 - val_binary_accuracy: 0.8387\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4392 - binary_accuracy: 0.8429 - val_loss: 0.4132 - val_binary_accuracy: 0.8387\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4323 - binary_accuracy: 0.8429 - val_loss: 0.4083 - val_binary_accuracy: 0.8710\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4271 - binary_accuracy: 0.8381 - val_loss: 0.4015 - val_binary_accuracy: 0.8710\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4206 - binary_accuracy: 0.8333 - val_loss: 0.3942 - val_binary_accuracy: 0.8387\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4143 - binary_accuracy: 0.8429 - val_loss: 0.3887 - val_binary_accuracy: 0.8387\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4093 - binary_accuracy: 0.8524 - val_loss: 0.3828 - val_binary_accuracy: 0.8387\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4038 - binary_accuracy: 0.8571 - val_loss: 0.3779 - val_binary_accuracy: 0.8387\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3982 - binary_accuracy: 0.8571 - val_loss: 0.3746 - val_binary_accuracy: 0.8387\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3942 - binary_accuracy: 0.8571 - val_loss: 0.3700 - val_binary_accuracy: 0.8387\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3899 - binary_accuracy: 0.8619 - val_loss: 0.3652 - val_binary_accuracy: 0.8387\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3852 - binary_accuracy: 0.8571 - val_loss: 0.3612 - val_binary_accuracy: 0.8387\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3811 - binary_accuracy: 0.8571 - val_loss: 0.3573 - val_binary_accuracy: 0.8710\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3769 - binary_accuracy: 0.8619 - val_loss: 0.3552 - val_binary_accuracy: 0.8387\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3733 - binary_accuracy: 0.8619 - val_loss: 0.3519 - val_binary_accuracy: 0.8387\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3696 - binary_accuracy: 0.8619 - val_loss: 0.3490 - val_binary_accuracy: 0.8387\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3660 - binary_accuracy: 0.8571 - val_loss: 0.3480 - val_binary_accuracy: 0.8387\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3635 - binary_accuracy: 0.8667 - val_loss: 0.3449 - val_binary_accuracy: 0.8387\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3599 - binary_accuracy: 0.8619 - val_loss: 0.3442 - val_binary_accuracy: 0.8387\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3569 - binary_accuracy: 0.8810 - val_loss: 0.3418 - val_binary_accuracy: 0.8387\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3540 - binary_accuracy: 0.8714 - val_loss: 0.3395 - val_binary_accuracy: 0.8387\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3511 - binary_accuracy: 0.8762 - val_loss: 0.3375 - val_binary_accuracy: 0.8387\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3483 - binary_accuracy: 0.8714 - val_loss: 0.3359 - val_binary_accuracy: 0.8387\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3456 - binary_accuracy: 0.8762 - val_loss: 0.3350 - val_binary_accuracy: 0.8387\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3432 - binary_accuracy: 0.8857 - val_loss: 0.3335 - val_binary_accuracy: 0.8387\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3410 - binary_accuracy: 0.8810 - val_loss: 0.3326 - val_binary_accuracy: 0.8387\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3384 - binary_accuracy: 0.8857 - val_loss: 0.3309 - val_binary_accuracy: 0.8387\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3357 - binary_accuracy: 0.8857 - val_loss: 0.3304 - val_binary_accuracy: 0.8710\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3339 - binary_accuracy: 0.8810 - val_loss: 0.3311 - val_binary_accuracy: 0.8710\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3314 - binary_accuracy: 0.8857 - val_loss: 0.3319 - val_binary_accuracy: 0.8387\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3294 - binary_accuracy: 0.8857 - val_loss: 0.3304 - val_binary_accuracy: 0.8710\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3272 - binary_accuracy: 0.8857 - val_loss: 0.3295 - val_binary_accuracy: 0.8710\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3254 - binary_accuracy: 0.8810 - val_loss: 0.3278 - val_binary_accuracy: 0.8710\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3231 - binary_accuracy: 0.8762 - val_loss: 0.3268 - val_binary_accuracy: 0.8710\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3214 - binary_accuracy: 0.8762 - val_loss: 0.3269 - val_binary_accuracy: 0.8710\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3196 - binary_accuracy: 0.8857 - val_loss: 0.3257 - val_binary_accuracy: 0.8710\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3176 - binary_accuracy: 0.8857 - val_loss: 0.3243 - val_binary_accuracy: 0.8710\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3162 - binary_accuracy: 0.8762 - val_loss: 0.3232 - val_binary_accuracy: 0.8710\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3146 - binary_accuracy: 0.8810 - val_loss: 0.3236 - val_binary_accuracy: 0.8710\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3126 - binary_accuracy: 0.8857 - val_loss: 0.3236 - val_binary_accuracy: 0.9032\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3114 - binary_accuracy: 0.8810 - val_loss: 0.3223 - val_binary_accuracy: 0.9032\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3099 - binary_accuracy: 0.8810 - val_loss: 0.3218 - val_binary_accuracy: 0.9032\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3082 - binary_accuracy: 0.8810 - val_loss: 0.3219 - val_binary_accuracy: 0.9032\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3065 - binary_accuracy: 0.8810 - val_loss: 0.3230 - val_binary_accuracy: 0.8710\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3053 - binary_accuracy: 0.8857 - val_loss: 0.3233 - val_binary_accuracy: 0.8710\n",
      "Elapsed time: 3.947894 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16  # Large: speed and faster, small: generalization\n",
    "epochs = 200\n",
    "hidden_layer_unit = 16  # Total number of 300 record! First try is 8!\n",
    "hidden_layer_activation = 'relu'  # This is a hidden layer, Relu is good choice for this section\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(X_train_scl.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=hidden_layer_unit, activation=hidden_layer_activation),\n",
    "    tf.keras.layers.Dense(units=hidden_layer_unit, activation=hidden_layer_activation),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    # This is classification output -> Sigmoid or softmax are the best chooses\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "history2 ,early_stop2 = build_model(model2, epochs, loss, optimizer, metric ,batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:48.809901Z",
     "start_time": "2023-05-31T19:28:44.804005400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                416       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32)               128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,761\n",
      "Trainable params: 1,633\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "27/27 [==============================] - 1s 11ms/step - loss: 3.5332 - binary_accuracy: 0.7143 - val_loss: 3.2442 - val_binary_accuracy: 0.9032\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 3.3477 - binary_accuracy: 0.7571 - val_loss: 3.1448 - val_binary_accuracy: 0.9355\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 3.2369 - binary_accuracy: 0.7905 - val_loss: 3.0650 - val_binary_accuracy: 0.9032\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 3.1750 - binary_accuracy: 0.7810 - val_loss: 3.0054 - val_binary_accuracy: 0.8710\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 3.1114 - binary_accuracy: 0.7810 - val_loss: 2.9362 - val_binary_accuracy: 0.9032\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 3.0230 - binary_accuracy: 0.8333 - val_loss: 2.8714 - val_binary_accuracy: 0.9355\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.9615 - binary_accuracy: 0.8095 - val_loss: 2.8059 - val_binary_accuracy: 0.9032\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.8759 - binary_accuracy: 0.8048 - val_loss: 2.7445 - val_binary_accuracy: 0.9355\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.8192 - binary_accuracy: 0.8286 - val_loss: 2.6914 - val_binary_accuracy: 0.9032\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.8161 - binary_accuracy: 0.7810 - val_loss: 2.6315 - val_binary_accuracy: 0.8710\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.6959 - binary_accuracy: 0.8000 - val_loss: 2.5674 - val_binary_accuracy: 0.9032\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.6532 - binary_accuracy: 0.8143 - val_loss: 2.5150 - val_binary_accuracy: 0.9032\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.6113 - binary_accuracy: 0.8048 - val_loss: 2.4557 - val_binary_accuracy: 0.9032\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.5537 - binary_accuracy: 0.7857 - val_loss: 2.4022 - val_binary_accuracy: 0.8710\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.4898 - binary_accuracy: 0.8238 - val_loss: 2.3471 - val_binary_accuracy: 0.9032\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.3997 - binary_accuracy: 0.8429 - val_loss: 2.2911 - val_binary_accuracy: 0.9032\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.3588 - binary_accuracy: 0.8000 - val_loss: 2.2366 - val_binary_accuracy: 0.9032\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.3217 - binary_accuracy: 0.8143 - val_loss: 2.1958 - val_binary_accuracy: 0.9032\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.2688 - binary_accuracy: 0.8190 - val_loss: 2.1380 - val_binary_accuracy: 0.9032\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.2680 - binary_accuracy: 0.7857 - val_loss: 2.0924 - val_binary_accuracy: 0.9032\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.1617 - binary_accuracy: 0.8381 - val_loss: 2.1146 - val_binary_accuracy: 0.8710\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.1843 - binary_accuracy: 0.7762 - val_loss: 2.0273 - val_binary_accuracy: 0.9355\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.0968 - binary_accuracy: 0.8143 - val_loss: 1.9702 - val_binary_accuracy: 0.8387\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.0861 - binary_accuracy: 0.7905 - val_loss: 1.9394 - val_binary_accuracy: 0.9032\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.0156 - binary_accuracy: 0.8048 - val_loss: 1.8952 - val_binary_accuracy: 0.9032\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.9179 - binary_accuracy: 0.8190 - val_loss: 1.8439 - val_binary_accuracy: 0.9032\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.9305 - binary_accuracy: 0.8190 - val_loss: 1.8119 - val_binary_accuracy: 0.8710\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.8341 - binary_accuracy: 0.8238 - val_loss: 1.7626 - val_binary_accuracy: 0.8710\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.8249 - binary_accuracy: 0.8143 - val_loss: 1.7373 - val_binary_accuracy: 0.8387\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.8354 - binary_accuracy: 0.8095 - val_loss: 1.6874 - val_binary_accuracy: 0.9032\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.8026 - binary_accuracy: 0.8095 - val_loss: 1.6531 - val_binary_accuracy: 0.9032\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.6814 - binary_accuracy: 0.8381 - val_loss: 1.6171 - val_binary_accuracy: 0.8710\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.6556 - binary_accuracy: 0.8476 - val_loss: 1.5878 - val_binary_accuracy: 0.8387\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.5714 - binary_accuracy: 0.8571 - val_loss: 1.5469 - val_binary_accuracy: 0.8710\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.6323 - binary_accuracy: 0.7952 - val_loss: 1.5171 - val_binary_accuracy: 0.8710\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.6376 - binary_accuracy: 0.7810 - val_loss: 1.4577 - val_binary_accuracy: 0.9032\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.4910 - binary_accuracy: 0.8476 - val_loss: 1.4375 - val_binary_accuracy: 0.8710\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.5024 - binary_accuracy: 0.8333 - val_loss: 1.4137 - val_binary_accuracy: 0.9032\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.4655 - binary_accuracy: 0.8143 - val_loss: 1.3730 - val_binary_accuracy: 0.8710\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.4315 - binary_accuracy: 0.8286 - val_loss: 1.3521 - val_binary_accuracy: 0.9355\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.4564 - binary_accuracy: 0.7762 - val_loss: 1.3469 - val_binary_accuracy: 0.9032\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.4474 - binary_accuracy: 0.8190 - val_loss: 1.3003 - val_binary_accuracy: 0.9032\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.3690 - binary_accuracy: 0.8238 - val_loss: 1.2784 - val_binary_accuracy: 0.9032\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.3495 - binary_accuracy: 0.8000 - val_loss: 1.2479 - val_binary_accuracy: 0.9032\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.3279 - binary_accuracy: 0.8190 - val_loss: 1.2467 - val_binary_accuracy: 0.8710\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.2944 - binary_accuracy: 0.8238 - val_loss: 1.2067 - val_binary_accuracy: 0.8710\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.2830 - binary_accuracy: 0.8000 - val_loss: 1.2397 - val_binary_accuracy: 0.8387\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.2532 - binary_accuracy: 0.8238 - val_loss: 1.1941 - val_binary_accuracy: 0.8710\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.2228 - binary_accuracy: 0.8238 - val_loss: 1.1285 - val_binary_accuracy: 0.8387\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.2132 - binary_accuracy: 0.8238 - val_loss: 1.1151 - val_binary_accuracy: 0.8387\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.1795 - binary_accuracy: 0.8429 - val_loss: 1.0846 - val_binary_accuracy: 0.8710\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.1490 - binary_accuracy: 0.8238 - val_loss: 1.0497 - val_binary_accuracy: 0.8710\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0945 - binary_accuracy: 0.7952 - val_loss: 1.0396 - val_binary_accuracy: 0.8387\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.0784 - binary_accuracy: 0.8000 - val_loss: 1.0365 - val_binary_accuracy: 0.8065\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0423 - binary_accuracy: 0.8238 - val_loss: 1.0329 - val_binary_accuracy: 0.8065\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0539 - binary_accuracy: 0.8048 - val_loss: 0.9640 - val_binary_accuracy: 0.8710\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0380 - binary_accuracy: 0.8048 - val_loss: 0.9362 - val_binary_accuracy: 0.8387\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0060 - binary_accuracy: 0.8048 - val_loss: 0.9200 - val_binary_accuracy: 0.9032\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 1.0335 - binary_accuracy: 0.7857 - val_loss: 0.9451 - val_binary_accuracy: 0.8387\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.9434 - binary_accuracy: 0.8333 - val_loss: 0.9339 - val_binary_accuracy: 0.8387\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.9901 - binary_accuracy: 0.8095 - val_loss: 0.9216 - val_binary_accuracy: 0.8710\n",
      "Elapsed time: 5.651716 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8  # Large: speed and faster, small: generalization\n",
    "epochs = 200\n",
    "hidden_layer_unit = 32\n",
    "hidden_layer_activation = 'selu'\n",
    "\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(X_train_scl.shape[1],)),\n",
    "\n",
    "    tf.keras.layers.Dense(units=hidden_layer_unit, kernel_initializer=tf.initializers.LecunNormal(),\n",
    "                           kernel_regularizer=tf.keras.regularizers.l1_l2()),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(hidden_layer_activation),\n",
    "\n",
    "    tf.keras.layers.Dense(units=hidden_layer_unit, kernel_initializer=tf.initializers.LecunNormal(),\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2()),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(hidden_layer_activation),\n",
    "\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.01, nesterov=True)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "history_3, early_stop_3 = build_model(model_3, epochs, loss, optimizer, metric,\n",
    "                                                          batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:31:12.809818400Z",
     "start_time": "2023-05-31T19:31:07.043855300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Speed Model__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                208       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 497\n",
      "Trainable params: 497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "7/7 [==============================] - 1s 31ms/step - loss: 0.6410 - binary_accuracy: 0.6762 - val_loss: 0.5820 - val_binary_accuracy: 0.6129\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6117 - binary_accuracy: 0.7000 - val_loss: 0.5508 - val_binary_accuracy: 0.6452\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5869 - binary_accuracy: 0.7286 - val_loss: 0.5237 - val_binary_accuracy: 0.6774\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5636 - binary_accuracy: 0.7476 - val_loss: 0.5013 - val_binary_accuracy: 0.7419\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5437 - binary_accuracy: 0.7524 - val_loss: 0.4806 - val_binary_accuracy: 0.7742\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5258 - binary_accuracy: 0.7619 - val_loss: 0.4625 - val_binary_accuracy: 0.8065\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5093 - binary_accuracy: 0.7619 - val_loss: 0.4476 - val_binary_accuracy: 0.8065\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4940 - binary_accuracy: 0.7905 - val_loss: 0.4334 - val_binary_accuracy: 0.8065\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4811 - binary_accuracy: 0.7905 - val_loss: 0.4202 - val_binary_accuracy: 0.8065\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4680 - binary_accuracy: 0.7905 - val_loss: 0.4088 - val_binary_accuracy: 0.8065\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4558 - binary_accuracy: 0.7952 - val_loss: 0.3981 - val_binary_accuracy: 0.8065\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4443 - binary_accuracy: 0.8000 - val_loss: 0.3887 - val_binary_accuracy: 0.8065\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4346 - binary_accuracy: 0.8048 - val_loss: 0.3796 - val_binary_accuracy: 0.8065\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4247 - binary_accuracy: 0.8095 - val_loss: 0.3713 - val_binary_accuracy: 0.8065\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4155 - binary_accuracy: 0.8095 - val_loss: 0.3641 - val_binary_accuracy: 0.8065\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4063 - binary_accuracy: 0.8143 - val_loss: 0.3577 - val_binary_accuracy: 0.8065\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3989 - binary_accuracy: 0.8190 - val_loss: 0.3512 - val_binary_accuracy: 0.8065\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3915 - binary_accuracy: 0.8190 - val_loss: 0.3456 - val_binary_accuracy: 0.8065\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3847 - binary_accuracy: 0.8190 - val_loss: 0.3413 - val_binary_accuracy: 0.8065\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3786 - binary_accuracy: 0.8286 - val_loss: 0.3370 - val_binary_accuracy: 0.8065\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3726 - binary_accuracy: 0.8286 - val_loss: 0.3331 - val_binary_accuracy: 0.8065\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3664 - binary_accuracy: 0.8333 - val_loss: 0.3295 - val_binary_accuracy: 0.8065\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3613 - binary_accuracy: 0.8381 - val_loss: 0.3269 - val_binary_accuracy: 0.8065\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3564 - binary_accuracy: 0.8429 - val_loss: 0.3240 - val_binary_accuracy: 0.8065\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3516 - binary_accuracy: 0.8524 - val_loss: 0.3212 - val_binary_accuracy: 0.8387\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3473 - binary_accuracy: 0.8524 - val_loss: 0.3187 - val_binary_accuracy: 0.8387\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3432 - binary_accuracy: 0.8571 - val_loss: 0.3166 - val_binary_accuracy: 0.8387\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3392 - binary_accuracy: 0.8429 - val_loss: 0.3150 - val_binary_accuracy: 0.8387\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3358 - binary_accuracy: 0.8429 - val_loss: 0.3139 - val_binary_accuracy: 0.8387\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3321 - binary_accuracy: 0.8476 - val_loss: 0.3139 - val_binary_accuracy: 0.8387\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3288 - binary_accuracy: 0.8476 - val_loss: 0.3134 - val_binary_accuracy: 0.8710\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3258 - binary_accuracy: 0.8571 - val_loss: 0.3126 - val_binary_accuracy: 0.8710\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3226 - binary_accuracy: 0.8571 - val_loss: 0.3124 - val_binary_accuracy: 0.8710\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3194 - binary_accuracy: 0.8524 - val_loss: 0.3120 - val_binary_accuracy: 0.8710\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3169 - binary_accuracy: 0.8571 - val_loss: 0.3113 - val_binary_accuracy: 0.8710\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3138 - binary_accuracy: 0.8571 - val_loss: 0.3109 - val_binary_accuracy: 0.8710\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3111 - binary_accuracy: 0.8619 - val_loss: 0.3104 - val_binary_accuracy: 0.8710\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3091 - binary_accuracy: 0.8619 - val_loss: 0.3100 - val_binary_accuracy: 0.8710\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3064 - binary_accuracy: 0.8619 - val_loss: 0.3103 - val_binary_accuracy: 0.8710\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3043 - binary_accuracy: 0.8619 - val_loss: 0.3103 - val_binary_accuracy: 0.8710\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3022 - binary_accuracy: 0.8619 - val_loss: 0.3102 - val_binary_accuracy: 0.8710\n",
      "Elapsed time: 2.319187 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Large: speed and faster, small: generalization\n",
    "epochs = 200\n",
    "hidden_layer_unit = 32\n",
    "hidden_layer_activation = 'leaky_relu'\n",
    "\n",
    "model_best = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(X_train_scl.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=16, activation=hidden_layer_activation,\n",
    "                          kernel_initializer=tf.initializers.HeNormal()),\n",
    "    tf.keras.layers.Dense(units=16, activation=hidden_layer_activation,\n",
    "                          kernel_initializer=tf.initializers.HeNormal()),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "history_best, early_stop_speed = build_model(model_best, epochs, loss, optimizer, metric, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:28:55.483321800Z",
     "start_time": "2023-05-31T19:28:53.108132700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And this model seems to have a good balance between all the provided models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (Bonus point) Save your model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T17:49:01.662709500Z",
     "start_time": "2023-05-21T17:49:01.604408900Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Code is implemented in build_model function as model.save()\n",
    "this code is same as tf.keras.Model.save\n",
    "for loading that we call tf.keras.models.load_model('saved_model/my_model') function"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
