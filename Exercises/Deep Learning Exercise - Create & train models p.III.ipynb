{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["LqroyD_ff4rK","IZwNT-5R7kMv","5B_xw48aX2VI","Lo561zAMX4Z9","TmK_gxJzhlfc","z7iR1iWnhoJI","WLeAzKiFX6Dd"],"authorship_tag":"ABX9TyOW815VGaI6rcIjlYSaQB7E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["*This Notebook has been created by PALISSON Antoine.*<br>\n"],"metadata":{"id":"4FMp9gtS-2LO"}},{"cell_type":"code","source":["# Basic packages\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Tensorflow\n","import tensorflow as tf"],"metadata":{"id":"XxHArnZntSDY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"LqroyD_ff4rK"}},{"cell_type":"markdown","source":["In this exercise, we will use an income dataset: `'adult.csv'`.\n","\n","**<font color='blue'>1. Load the dataset and show its content.**"],"metadata":{"id":"dzUjn-93JsS-"}},{"cell_type":"code","source":[],"metadata":{"id":"tFkjLKgAFB2J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>2. Separate the features and the label (the `income` variable).**"],"metadata":{"id":"E4jh699yJx6O"}},{"cell_type":"code","source":[],"metadata":{"id":"xmJKMSKrFClr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>3. Create a test and validation set.**"],"metadata":{"id":"W7-OhyVTJ6pN"}},{"cell_type":"code","source":[],"metadata":{"id":"mONQ4wzfFDJM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>4. Preprocess the data.**\n","\n","*Tips: The preprocessing is different for the numerical and the categorical data*"],"metadata":{"id":"g7YG87IoKD8-"}},{"cell_type":"code","source":[],"metadata":{"id":"657TDRTdFDid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Benchmark"],"metadata":{"id":"IZwNT-5R7kMv"}},{"cell_type":"markdown","source":["In this part, you will build a **benchmark** to compare the effect of the optimizer algorithms and the learning rate.\n","\n","Here is a small function that should be run before every training to reset the seeds.<br> This ensures that the **randomness is always the same during training**, hence making the results comparable."],"metadata":{"id":"wnjkUNIR_mLV"}},{"cell_type":"code","source":["def reset_seeds():\n","    os.environ['PYTHONHASHSEED']=str(2)\n","    tf.random.set_seed(2)\n","    np.random.seed(2)\n","    random.seed(2)"],"metadata":{"id":"M6fsxGlO833E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>1. Build a model architecture that contains 2 hidden layers with 16 neurons and a ReLU activation.<br> The remaining parameters are set to default.<br> Add the appropriate output layer.**\n","\n","*Tips: You should run the reset_seeds() function at the beginning of the cell.*"],"metadata":{"id":"Ce-4e8mjAS-l"}},{"cell_type":"code","source":[],"metadata":{"id":"1Q9JEwHZFFUF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>2. Compile the model with the SGD optimizer, a learning rate of 0.005 and the appropriate loss.**"],"metadata":{"id":"9kbgYv7pBLg8"}},{"cell_type":"code","source":[],"metadata":{"id":"-xiYL_NmFFrI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>3. Train the model for 10 epochs with a batch size of 32.**\n","\n","*Tips: Don't forget the validation data !*"],"metadata":{"id":"l4UUf99qBck1"}},{"cell_type":"code","source":[],"metadata":{"id":"NUZJMQ3iFGJc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>4. Display the loss curves (training & validation).** "],"metadata":{"id":"uLGYOV2lBx1E"}},{"cell_type":"code","source":[],"metadata":{"id":"s3iFeXWhFGsN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimizers"],"metadata":{"id":"5B_xw48aX2VI"}},{"cell_type":"markdown","source":["Momentum is a hyperparameter that controls the influence of previous gradients on the current update. It can be added to the SGD algorithm.\n","\n","A higher momentum value means that the optimizer will take more of the previous gradients into account when calculating the current update, which can help the optimizer to converge faster and avoid getting stuck in local minima. A lower momentum value can help the optimizer to explore the parameter space more thoroughly, but may also slow down convergence.\n","\n","The most common values for momentum are between 0.9 and 0.99.\n","\n","**<font color='blue'>1. Using the same architecture as the benchmark model, add some momentum to the SGD optimizer using its `momentum` parameter.**"],"metadata":{"id":"4NJpoppYbZDE"}},{"cell_type":"code","source":[],"metadata":{"id":"Jm7k1qJGFIJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>2. Compare the result with the benchmark.**"],"metadata":{"id":"wY_VsdNAc1sN"}},{"cell_type":"code","source":[],"metadata":{"id":"ifJf3a4TFIsq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>3. Add both the momentum and the Nesterov Momentum (`nesterov=True`) to the SGD optimizer.<br> Train the model and compare the results.**"],"metadata":{"id":"dULI1EsBdV9v"}},{"cell_type":"code","source":[],"metadata":{"id":"u2Hf2iWSFJtL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In TensorFlow, the Adam optimizer is implemented as `tf.keras.optimizers.Adam()`.<br> The beta_1 and the beta_2 parameters refers to the beta_z and beta_s respectively (in the lesson).\n","\n","\n","**<font color='blue'>4. Using the same architecture as the benchmark model, replace the SGD optimizer by the Adam optimizer.<br> Compare the result.**"],"metadata":{"id":"E8BZP1gBekCp"}},{"cell_type":"code","source":[],"metadata":{"id":"oFq7GgIUFKX9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Learning Rate"],"metadata":{"id":"Lo561zAMX4Z9"}},{"cell_type":"markdown","source":["## Fixed learning rate"],"metadata":{"id":"TmK_gxJzhlfc"}},{"cell_type":"markdown","source":["**<font color='blue'>1. Using the benchmark architecture and the Adam optimizer, try the following learning rate values:**<br>\n","**<font color='blue'>0.00001 / 0.0001 / 0.001 / 0.01 / 0.1 / 1**<br>\n","**<font color='blue'>Compare the losses**"],"metadata":{"id":"RlhpHXlshqna"}},{"cell_type":"code","source":[],"metadata":{"id":"Qcv5mvbDFLz0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Learning rate scheduler"],"metadata":{"id":"z7iR1iWnhoJI"}},{"cell_type":"markdown","source":["TensorFlow Keras provides several learning rate schedulers that can be used to adjust the learning rate during training including:\n","\n","\n","*   **Exponential** decay function - `tf.keras.optimizers.schedules.ExponentialDecay` - [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay)\n","    * `decay_rate` (typically ranges between 0.1 and 0.5) parameter controls the rate at which the learning rate decreases\n","    * `decay_steps` parameter (typically ranges between 1000 and 10000 but depends on the number of instances) controls the instance-based frequency at which the learning rate is decreased\n","*  **Cosine** decay function - `tf.keras.optimizers.schedules.CosineDecay` -  [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecayRestarts)\n","    * `alpha` (typically ranges between 0.0 and 1.0) sets the amplitude of the oscillation.\n","*   **Polynomial** decay function - `tf.keras.optimizers.schedules.PolynomialDecay` - [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay)\n","    * `power` (typically ranges between 0.5 and 2.0) sets the power of the polynomial.\n","*   **Power** decay function - `tf.keras.optimizers.schedules.InverseTimeDecay` - [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/InverseTimeDecay)\n","*   **Piecewise** decay function - `tf.keras.optimizers.schedules.PiecewiseConstantDecay`: [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PiecewiseConstantDecay)\n","    * `boundaries`: A list of non-decreasing integers that define the steps or epochs at which to apply a new learning rate.\n","    * `values`: A list of length `len(boundaries) + 1` that specifies the learning rates to apply at each boundary. The first value corresponds to the initial learning rate, and subsequent values correspond to the learning rates after each boundary.\n","*   **Performance** decay - `tf.keras.callbacks.ReduceLROnPlateau` - [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)\n","    * `factor`: (typically ranges between 0.1 and 0.5) sets the factor by which the learning rate will be reduced.\n","\n","    * `patience`: (typically ranges between 3 and 10) sets the number of epochs to wait before reducing the learning rate when the monitored metric has stopped improving.\n","\n","    * `min_delta`: sets the minimum change in the monitored metric that is considered an improvement.\n","\n","    * `cooldown`: sets the number of epochs to wait after reducing the learning rate before resuming normal operation.\n","\n","There is also a callback that allows you to define a function to schedule the learning rate based on the epoch number or the current iteration number `tf.keras.callbacks.LearningRateScheduler` [Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule)"],"metadata":{"id":"B_PRdnyvjk41"}},{"cell_type":"markdown","source":["**<font color='blue'>1. Use an piecewise decay learning rate scheduler with three boundaries (i.e. four different learning rates) using the benchmark architecture and the Adam optimizer.**"],"metadata":{"id":"uHJn6QE9lFQk"}},{"cell_type":"code","source":[],"metadata":{"id":"9CfHF-NthpiT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>2. Do the same with an exponential learning rate scheduler.<br> Compare the results.**"],"metadata":{"id":"eZAIMkQmGVOq"}},{"cell_type":"code","source":[],"metadata":{"id":"lSZlW8zQGYUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**<font color='blue'>3. Do the same with a performance learning rate scheduler.<br> Compare the results.**"],"metadata":{"id":"huyU8h9A0DFt"}},{"cell_type":"code","source":[],"metadata":{"id":"Qtq3FH0s0b1H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparametrization"],"metadata":{"id":"WLeAzKiFX6Dd"}},{"cell_type":"markdown","source":["There are several Python libraries available for tuning the hyperparameters of a neural network including:\n","* Keras Tuner - [Documentation](https://keras.io/keras_tuner/)\n","* Optuna - [Documentation](https://optuna.readthedocs.io/en/stable/index.html)\n","* Ray Tune - [Documentation](https://docs.ray.io/en/latest/tune/index.html)\n","* Hyperopt - [Documentation](http://hyperopt.github.io/hyperopt/)\n","\n","These libraries have similar APIs and can be used in a similar way. Here's a general method for using these libraries for hyperparameter tuning:\n","\n","* **Define the search space**: *The first step is to define the search space for your hyperparameters. This includes specifying the range of values that each hyperparameter can take. For example, you can define a search space that includes the learning rate, number of hidden layers, batch size, and other hyperparameters.*\n","\n","* **Define the objective function**: *The next step is to define the objective function that you want to optimize. This is typically the performance metric of your model, such as accuracy or loss. You'll need to train and evaluate your model for each combination of hyperparameters and return the performance metric.*\n","\n","* **Choose a search algorithm**: *Next, you'll need to choose a search algorithm to explore the search space and find the optimal hyperparameters. The available algorithms vary depending on the library you choose, but popular choices include random search, grid search, Bayesian optimization, and other techniques.*\n","\n","* **Run the search**: *Once you've defined the search space, objective function, and search algorithm, you can start the search. The library will run multiple experiments, each with a different set of hyperparameters, and evaluate the performance of your model. After several iterations, the library will converge on the optimal set of hyperparameters.*\n","\n","* **Evaluate the best model**: *Finally, once the search is complete, you can evaluate the performance of the best model using the optimal hyperparameters.*\n","\n","**<font color='blue'>Using one of these library, find the best hyperparameters for the `adult.csv` dataset.**\n","\n","*Tips: You should try different combinations of number of hidden layers, number of neurons, activation functions, weight initialization, optimizer, learning rate, batch size, regularization techniques (norms, dropout, batch ..) and so on. <br> You can also treat your preprocessing choices as hyperparameters !*"],"metadata":{"id":"I5nOlpg61CUC"}},{"cell_type":"code","source":[],"metadata":{"id":"wvO3yWFwX-Ce"},"execution_count":null,"outputs":[]}]}