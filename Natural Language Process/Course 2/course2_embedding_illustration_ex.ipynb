{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPeikwq1xZaQ"
   },
   "source": [
    "The goal is to check that the vector result of *king - man + woman* is close to *queen* vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LfCrTRvxoa7"
   },
   "source": [
    "## Try with a spaCy pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "762mKs9CxSdp",
    "ExecuteTime": {
     "end_time": "2023-06-12T12:55:27.627858500Z",
     "start_time": "2023-06-12T12:55:16.560188200Z"
    }
   },
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "from scipy import spatial\n",
    "# we dowload a nlp english model (with a pre-trained 300-dimension embedding) \n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.load('en_core_web_md')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2mâœ” Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVvKiD8zyfL_"
   },
   "source": [
    "spaCy allows to compute directly a pre-trained 300-dimension embedding for every word\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i2z6QyoInHBJ",
    "ExecuteTime": {
     "end_time": "2023-06-12T12:55:41.137503200Z",
     "start_time": "2023-06-12T12:55:41.105504100Z"
    }
   },
   "source": [
    "king = nlp.vocab['king']\n",
    "king.vector"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.1296e-01, -4.1865e+00, -1.8453e+00,  3.0781e-01,  2.4956e+00,\n        9.6267e-01, -1.8161e+00,  4.4655e+00, -2.8210e+00,  9.7090e-01,\n        1.3542e+01,  4.3195e-01, -5.3098e+00,  4.7098e+00,  2.9030e+00,\n        1.5588e+00,  6.0064e+00, -3.0345e+00,  1.0626e+00, -7.7197e-01,\n       -5.4771e+00, -9.7380e-01, -4.4345e+00,  5.8367e+00,  2.4302e+00,\n       -3.9408e+00, -9.1862e-01, -4.9124e+00,  1.4591e+00, -7.2772e-01,\n        3.4957e+00, -4.0077e+00, -1.8354e+00, -4.1052e+00,  4.9211e+00,\n       -9.7053e-01,  1.9223e+00,  5.2605e+00,  1.6086e+00,  7.1328e-01,\n       -1.2146e+00, -1.9869e+00,  8.0265e-01,  2.9298e+00,  7.2985e-01,\n       -6.2892e-01, -1.7082e+00,  1.9893e+00,  4.7529e-01,  3.2264e+00,\n       -3.9215e+00,  4.6556e+00,  1.3475e+00, -1.0979e+00, -3.0365e+00,\n        1.5815e+00,  2.2835e+00, -4.0616e+00,  2.5730e+00,  4.0618e+00,\n        9.5438e-01, -6.2563e+00,  5.6463e+00, -3.8933e+00,  4.4076e+00,\n        2.0517e+00, -6.6906e+00, -6.9448e+00,  6.0371e+00,  9.3081e-01,\n        1.5180e+00,  2.3974e+00, -3.8043e+00, -4.3941e+00, -3.6979e+00,\n        2.9489e+00, -8.9735e+00,  9.5273e+00, -6.4149e-01,  2.2565e+00,\n       -7.2062e+00, -1.0078e+00, -4.4381e+00,  2.0424e+00, -6.6736e-01,\n        4.3500e+00, -1.6199e+00,  3.1975e+00, -1.2065e+00, -6.5684e-01,\n        7.5759e-01, -1.6033e+00,  2.5450e+00, -5.4999e+00, -1.8909e+00,\n       -1.2985e-02,  2.6703e+00,  5.4623e-01, -2.4504e+00, -4.4326e-01,\n       -1.7250e+00,  9.1585e-01,  7.5243e+00, -5.8451e-01,  3.4550e+00,\n        3.4817e+00, -4.1599e+00, -5.5125e-01,  2.7681e-02, -3.1687e+00,\n       -4.8459e+00,  7.9108e+00, -1.7062e+00, -2.6731e+00,  9.7841e+00,\n        3.8851e+00, -3.7930e+00, -5.2979e-01,  6.6191e-01, -9.7232e-01,\n       -9.4692e-01, -4.4918e+00,  1.0932e+00, -4.3751e+00,  1.3182e-02,\n       -1.0243e+01,  4.7973e+00, -8.7426e+00,  2.5479e+00,  2.3454e+00,\n       -6.4140e+00,  7.3875e-01,  5.8565e+00, -2.5964e-01,  1.6558e+00,\n       -3.1353e+00, -6.6752e+00,  1.0550e+00,  1.7017e+00, -3.8360e+00,\n       -1.1980e+01, -1.3750e+00, -1.9261e+00,  3.1267e+00,  3.2874e+00,\n       -2.8928e+00, -1.0893e+01,  4.2848e+00, -4.0890e-02, -5.9565e-01,\n       -3.3473e-02,  1.6832e+00,  2.1454e-01,  7.2849e+00,  2.8116e+00,\n        2.5708e+00, -3.9823e-01, -1.7257e+00, -6.1063e+00, -4.2618e+00,\n       -3.3886e+00, -9.2663e+00,  1.7600e-01, -3.3873e-02, -3.7070e+00,\n       -9.1995e+00, -7.1594e+00, -6.0189e-01, -7.2560e-01,  1.5342e+00,\n        5.1083e+00,  2.4373e+00, -3.8012e+00, -2.1752e-01,  2.9503e+00,\n       -2.5551e+00,  4.9827e-01,  8.6823e-01, -4.3449e+00, -4.3821e+00,\n        3.4993e+00, -1.9518e+00,  2.2036e+00, -6.6526e-01,  7.1015e+00,\n        3.6784e+00,  2.6251e-01,  1.5379e+00, -8.1950e-01,  1.1065e+00,\n        3.3167e+00, -5.9392e+00, -4.0191e+00,  2.6496e+00,  2.3168e+00,\n       -8.5681e-02, -3.5059e+00,  1.5915e+00, -3.1831e-01,  6.9366e+00,\n        3.8439e+00,  9.4076e-01, -7.5424e+00,  2.7847e+00, -2.2814e+00,\n       -4.2487e+00, -2.6604e-01,  3.7954e+00, -3.6526e+00,  4.3823e+00,\n       -2.6506e+00,  3.5298e+00,  2.2597e+00,  6.3055e+00, -7.0194e-01,\n        4.1565e+00,  8.2306e+00,  5.7675e-01,  4.3596e-01, -8.8400e+00,\n       -3.0249e+00,  4.0032e+00,  2.4232e+00,  6.9885e+00, -2.5906e-01,\n       -4.2059e+00,  1.2643e+00,  1.0110e+01,  9.7016e-01,  2.2963e+00,\n       -1.2802e+00, -1.4447e+00, -3.4386e+00,  5.6555e+00,  3.3911e+00,\n        6.9418e+00, -6.8705e+00, -8.1536e-01, -7.2334e+00,  3.0509e+00,\n        8.7676e-01,  6.4216e+00, -3.1655e+00, -1.5308e+00, -1.1056e+00,\n       -5.0426e+00,  4.6801e+00,  4.6812e+00,  4.0401e+00, -3.7289e-01,\n        6.7437e-01, -8.6660e+00, -9.9656e+00,  2.4979e+00, -1.4783e-01,\n       -5.6301e+00,  4.5542e+00,  4.8165e+00, -2.2055e-01,  4.5169e+00,\n        1.7496e+00,  2.9019e-01, -1.1683e+00, -4.3981e-01,  2.3469e+00,\n       -4.3521e-02,  6.3715e-01,  5.8259e-01, -8.5701e+00,  4.6419e+00,\n        2.3809e+00, -1.9273e-01, -6.9772e+00,  7.6172e-01, -6.3895e-01,\n       -3.3769e+00,  6.1265e+00, -1.9695e+00, -2.3404e+00,  6.6789e+00,\n       -3.5265e+00, -3.3883e+00,  6.1372e+00,  4.5550e+00,  6.0957e+00,\n       -2.2007e-01,  6.2087e-01,  2.5527e+00, -4.5590e+00, -2.8429e+00,\n        2.0645e+00, -1.6221e+00, -2.8171e+00, -2.9680e+00,  1.3651e+00,\n        3.6137e+00, -3.2096e-01, -1.9346e+00, -4.8738e+00,  2.5565e+00],\n      dtype=float32)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jOknO0BRzSRy",
    "ExecuteTime": {
     "end_time": "2023-06-12T12:55:46.987424700Z",
     "start_time": "2023-06-12T12:55:46.970907Z"
    }
   },
   "source": [
    "king.vector.shape"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(300,)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "queen = nlp.vocab['queen'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "result = king - man + woman"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:07:04.563056200Z",
     "start_time": "2023-06-12T13:07:04.542051800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KWSerK8SsYMd",
    "ExecuteTime": {
     "end_time": "2023-06-12T13:03:17.578774Z",
     "start_time": "2023-06-12T13:03:17.551768600Z"
    }
   },
   "source": [
    "# Question 1: Compute the vector \"king - man + woman\" and try to show that the result is close to the vector representation of the word \"queen\" ;\n",
    "# a good way to do it is, for example, to find the 10 closest word (among the nlp.vocab words) from the results of \"king - man + woman\" and to show\n",
    "# that \"queen\" is one of them (if not the best)\n",
    "\n",
    "# The distance we need for that is the cosine similarity, it can be define from the spatial.distance.cosine function imported from the scipy library\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "# Start the exercice here\n",
    "# Hint: use a loop on nlp.vocab (all the words defined in spaCy vocabulary) ; for each \"word\" in the vocabulary you can check if the word has an embedding vector (\"word.has_vector\"), if the word is in\n",
    "# lower case (\"word.is_lower\") and is alphanumeric (\"word.is_alpha\"). Try to consider only the relevant words for the exercice\n",
    "# ??????"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.vocab.Vocab object at 0x00000219722611B0> has similarity of 0.8489541411399841\n",
      "<spacy.vocab.Vocab object at 0x00000219722611B0> has similarity of 0.07003621011972427\n",
      "<spacy.vocab.Vocab object at 0x00000219722611B0> has similarity of 0.30994713306427\n",
      "0.6178014278411865\n"
     ]
    }
   ],
   "source": [
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "for word in x:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))\n",
    "computed_similarities = sorted(computed_similarities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:13:36.799246Z",
     "start_time": "2023-06-12T13:13:36.777241300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdfvSzyFxsz5"
   },
   "source": [
    "## Try with a pretrained Word2Vec embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DnQ8h8IhJO4"
   },
   "source": [
    "**Important** To prevent RAM crash in the execution environment, please restart from here the running environment (Execution -> Restart the running environment)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wkPTJ9oLv_Hv"
   },
   "source": [
    "import gensim# Load pretrained vectors from Google\n",
    "from gensim.models import KeyedVectors"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvv6t5uyzffL"
   },
   "source": [
    "We load the pre-trained glove vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased embedding models (100-dimension embedding)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9Qm9ua_Tv_P6"
   },
   "source": [
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jczj7qXfv_TB"
   },
   "source": [
    "king = word_vectors['king']\n",
    "\n",
    "print(king)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k7vxg9DqzreT"
   },
   "source": [
    "king.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BhVZa8Ufv_bE"
   },
   "source": [
    "# Question 2: This time with the GoogleNews embedding model, try to show once again that \"king - man + woman\" is close to the vector representation of the word \"queen\" ;\n",
    "# Hint: There is a pre-defined function in the gensim \"word_vectors\" object (define just above) that allows to get this result quite easily\n",
    "\n",
    "# ??????????????"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQ0XN3j7i4Fk"
   },
   "source": [
    "## Try with fastText embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvKq5vbXi8up"
   },
   "source": [
    "**Important** To prevent RAM crash in the execution environment, please restart from here the running environment (Execution -> Restart the running environment)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wUCj7-bGjc73"
   },
   "source": [
    "#Download, extract and load Fasttext word embedding model\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "!gunzip /content/cc.en.300.bin.gz\n",
    "!pip install fasttext"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suVw4N4cjAXc"
   },
   "source": [
    "Load the english fastText model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0YH4W8m-i7P6"
   },
   "source": [
    "import fasttext \n",
    "\n",
    "model = fasttext.load_model(\"/content/cc.en.300.bin\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fNSYa7qm4gyN"
   },
   "source": [
    "model.get_word_vector(\"king\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXHl3VkijDWQ"
   },
   "source": [
    "It is possible to get directly the nearest neighbors of a specific word (or even n-gram)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rwxBjH-YjFg1"
   },
   "source": [
    "model.get_nearest_neighbors(\"king\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gPFj4l47jHx-"
   },
   "source": [
    "# Question 3: This time with the fastText embedding model, try to show once again that \"king - man + woman\" is close to the vector representation of the word \"queen\" ;\n",
    "# Hint: There is a pre-defined function in the fastText model, 'get_analogies', that allows to get this result quite easily"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
